{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47\n",
      "  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65\n",
      "  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83\n",
      "  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  60  61  62  63  64  65\n",
      "  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83\n",
      "  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149] TEST: [30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\n",
      " 55 56 57 58 59]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149] TEST: [60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84\n",
      " 85 86 87 88 89]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149] TEST: [ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119] TEST: [120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "[array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.00105557,  0.00502047,  0.00105557,  0.00105557,  0.00105557,\n",
      "         0.00105557,  0.00105557,  0.00105557,  0.00502047,  0.00105557,\n",
      "         0.00105557,  0.00105557,  0.00502047,  0.00502047,  0.00105557,\n",
      "         0.00105557,  0.00105557,  0.00105557,  0.00105557,  0.00105557,\n",
      "         0.00105557,  0.00105557,  0.00105557,  0.00105557,  0.00105557,\n",
      "         0.00502047,  0.00105557,  0.00105557,  0.00105557,  0.00105557]]), array([[ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
      "         0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  1. ,\n",
      "         1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ],\n",
      "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
      "         0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  1. ,\n",
      "         1.2,  1. ,  1. ,  1. ,  1. ,  1.3,  1. ,  1. ]]), array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.]]), array([[ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
      "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
      "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ,\n",
      "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ,\n",
      "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ,\n",
      "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ],\n",
      "       [ 1.07934466,  1.10740481,  0.96619545,  0.9387628 ,  1.04157266,\n",
      "         0.89805822,  0.97520701,  0.93217377,  0.8669234 ,  0.98231899,\n",
      "         2.10754348,  1.80823822,  2.06138211,  1.97539497,  2.15109514,\n",
      "         1.97002347,  1.32658275,  2.02375197,  1.98303997,  1.9885595 ,\n",
      "         1.72156828,  1.87725329,  1.98082941,  1.8196077 ,  2.00730036,\n",
      "         1.97588536,  1.89482287,  1.89067915,  1.7628556 ,  1.50665468]]), array([[ 2.        ,  2.        ,  2.        ,  2.        ,  2.        ,\n",
      "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ,\n",
      "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ,\n",
      "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ,\n",
      "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ,\n",
      "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ],\n",
      "       [ 1.84651267,  1.67077463,  1.81709689,  1.49292766,  1.74099085,\n",
      "         1.58214495,  1.47570753,  1.48989737,  1.78801461,  1.44076854,\n",
      "         1.67081325,  1.66462607,  1.84673528,  1.3481756 ,  1.42101135,\n",
      "         1.86165449,  1.92902441,  1.56998542,  1.48015216,  1.67821892,\n",
      "         1.91266026,  1.73733297,  1.6390193 ,  1.89509476,  1.97587355,\n",
      "         1.78364475,  1.58604059,  1.62687716,  1.84111608,  1.54817668]])]\n",
      "MSE: 7.86020248866e-06\n",
      "RMSE: 0.0028036052662\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model as lm \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "class model_fac:\n",
    "    def __init__(self):\n",
    "        self.name = \"create models for train\"\n",
    "    \n",
    "    def model_set(self):\n",
    "        gbdt = GradientBoostingRegressor()\n",
    "        rf = RandomForestRegressor()\n",
    "        ab = AdaBoostRegressor()\n",
    "        svr = SVR()\n",
    "        lm1 = lm.BayesianRidge()\n",
    "        modelset = {}\n",
    "        modelset['gbdt'] = gbdt\n",
    "        modelset['rf'] = rf\n",
    "        modelset['ab'] = ab\n",
    "        modelset['svr'] = svr\n",
    "        modelset['lm1'] = lm1\n",
    "        return modelset\n",
    "def data_fold(x_data,y_data,n_splits=3):\n",
    "    '''input : N*D N*1 array\n",
    "       output : TRAIN TEST LIST'''\n",
    "    folded = []\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    for train_index, test_index in kf.split(x_data):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = x_data[train_index], x_data[test_index]\n",
    "        y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "        temp = {}\n",
    "        y_train.shape = (len(y_train),1)\n",
    "        y_test.shape = (len(y_test),1)\n",
    "        temp['train'] = np.hstack((X_train,y_train))\n",
    "        temp['test'] = np.hstack((X_test,y_test))\n",
    "        folded.append(temp)\n",
    "    return folded\n",
    "\n",
    "def step1(folded,**model):\n",
    "    if len(folded) != len(model):\n",
    "        print('model number is ',len(model),'split dataset for ',len(folded))\n",
    "        return 0\n",
    "    keys = model.keys()\n",
    "    keys = [x for x in keys]\n",
    "    step1_result = [] \n",
    "    for i in range(len(folded)):\n",
    "        train = folded[i]['train']\n",
    "        x_train = train[:,:-1]\n",
    "        y_train = train[:,-1]#lie vec  [:,-1] hang vec\n",
    "        #print('x',x_train)\n",
    "        #print('y',y_train)\n",
    "        #train\n",
    "        model[keys[i]].fit(x_train,y_train)\n",
    "        #test\n",
    "        test = folded[i]['test']\n",
    "        x_test = test[:,:-1]\n",
    "        y_test = test[:,-1]#lie vec  [:,-1] hang vec\n",
    "        pred = model[keys[i]].predict(x_test)\n",
    "        #print(y_test)\n",
    "        #print(pred)\n",
    "        step1_result.append(np.stack((y_test,pred),axis=0))\n",
    "        #break\n",
    "    return step1_result\n",
    "\n",
    "model_num = 5\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "folded = data_fold(data.data,data.target,n_splits=model_num)\n",
    "#folded\n",
    "model = model_fac()\n",
    "modelset = model.model_set()\n",
    "\n",
    "result = step1(folded,**modelset)\n",
    "print(result)\n",
    "result = np.concatenate(result,axis=1)\n",
    "result.shape\n",
    "result\n",
    "y = result[:,0]\n",
    "pred = result[:,1]\n",
    "# 用scikit-learn计算MSE\n",
    "print(\"MSE:\",mean_squared_error(y, pred))\n",
    "# 用scikit-learn计算RMSE\n",
    "print (\"RMSE:\",np.sqrt(mean_squared_error(y, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重点是使用模型选择特征、利用GBDT制造特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47\n",
      "  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65\n",
      "  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83\n",
      "  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  60  61  62  63  64  65\n",
      "  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83\n",
      "  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149] TEST: [30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\n",
      " 55 56 57 58 59]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149] TEST: [60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84\n",
      " 85 86 87 88 89]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149] TEST: [ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119] TEST: [120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.13480388  0.19101658  0.02817339  0.17346949  0.06730667  0.00920173\n",
      "  0.00762614  0.00677044  0.00988459  0.00931691  0.09358164  0.05763183\n",
      "  0.03141438  0.17980233]\n",
      "\tCorr.\tLasso\tLinear reg\tRF\tRFE\tRidge\tStability\tgbdt\tMean\n",
      "0\t0.3\t0.79\t1.0\t0.69\t1.0\t0.77\t0.76\t0.69\t0.75\n",
      "1\t0.44\t0.83\t0.56\t1.0\t1.0\t0.75\t0.8\t0.82\t0.77\n",
      "2\t0.0\t0.0\t0.5\t0.12\t1.0\t0.05\t0.0\t1.0\t0.33\n",
      "3\t1.0\t1.0\t0.57\t0.9\t1.0\t1.0\t1.0\t0.68\t0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "#from minepy import MINE\n",
    "\n",
    "model_num = 5\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "folded = data_fold(data.data,data.target,n_splits=model_num)\n",
    "names = range(4)\n",
    "\n",
    "ranks = {}\n",
    "def rank_to_dict(ranks, names, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x, 2), ranks)\n",
    "    return dict(zip(names, ranks ))\n",
    "\n",
    "lr = LinearRegression(normalize=True)\n",
    "lr.fit(X, Y)\n",
    "ranks[\"Linear reg\"] = rank_to_dict(np.abs(lr.coef_), names)\n",
    "ridge = Ridge(alpha=7)\n",
    "ridge.fit(X, Y)\n",
    "ranks[\"Ridge\"] = rank_to_dict(np.abs(ridge.coef_), names)\n",
    "lasso = Lasso(alpha=.05)\n",
    "lasso.fit(X, Y)\n",
    "ranks[\"Lasso\"] = rank_to_dict(np.abs(lasso.coef_), names)\n",
    "rlasso = RandomizedLasso(alpha=0.04)\n",
    "rlasso.fit(X, Y)\n",
    "ranks[\"Stability\"] = rank_to_dict(np.abs(rlasso.scores_), names)\n",
    "#stop the search when 5 features are left (they will get equal scores)\n",
    "rfe = RFE(lr, n_features_to_select=5)\n",
    "rfe.fit(X,Y)\n",
    "ranks[\"RFE\"] = rank_to_dict(rfe.ranking_, names, order=-1)\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X,Y)\n",
    "print(rf.feature_importances_)\n",
    "ranks[\"RF\"] = rank_to_dict(rf.feature_importances_, names)\n",
    "f, pval  = f_regression(X, Y, center=True)\n",
    "ranks[\"Corr.\"] = rank_to_dict(f, names)\n",
    "gbdt = GradientBoostingRegressor().fit(X, Y)\n",
    "ranks[\"gbdt\"] = rank_to_dict(gbdt.feature_importances_, names)\n",
    "#mine = MINE()\n",
    "#mic_scores = []\n",
    "#for i in range(X.shape[1]):\n",
    "#    mine.compute_score(X[:,i], Y)\n",
    "#    m = mine.mic()\n",
    "#    mic_scores.append(m)\n",
    "#ranks[\"MIC\"] = rank_to_dict(mic_scores, names)\n",
    "r = {}\n",
    "for name in names:\n",
    "    r[name] = round(np.mean([ranks[method][name] for method in ranks.keys()]), 2)\n",
    "methods = sorted(ranks.keys())\n",
    "ranks[\"Mean\"] = r\n",
    "methods.append(\"Mean\")\n",
    "print (\"\\t%s\" % \"\\t\".join(methods))\n",
    "for name in names:\n",
    "    print(\"%s\\t%s\" % (name, \"\\t\".join(map(str, \n",
    "                        [ranks[method][name] for method in methods]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47\n",
      "  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65\n",
      "  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83\n",
      "  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  60  61  62  63  64  65\n",
      "  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83\n",
      "  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149] TEST: [30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\n",
      " 55 56 57 58 59]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149] TEST: [60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84\n",
      " 85 86 87 88 89]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149] TEST: [ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119] TEST: [120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00]\n",
      " [  1.05557110e-03   5.02046769e-03   1.05557110e-03   1.05557110e-03\n",
      "    1.05557110e-03   1.05557110e-03   1.05557110e-03   1.05557110e-03\n",
      "    5.02046769e-03   1.05557110e-03   1.05557110e-03   1.05557110e-03\n",
      "    5.02046769e-03   5.02046769e-03   1.05557110e-03   1.05557110e-03\n",
      "    1.05557110e-03   1.05557110e-03   1.05557110e-03   1.05557110e-03\n",
      "    1.05557110e-03   1.05557110e-03   1.05557110e-03   1.05557110e-03\n",
      "    1.05557110e-03   5.02046769e-03   1.05557110e-03   1.05557110e-03\n",
      "    1.05557110e-03   1.05557110e-03   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.40000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.10000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   2.00000000e+00   1.00000000e+00\n",
      "    1.75000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.75000000e+00   2.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   2.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.16609681e+00   1.27049748e+00\n",
      "    1.04841736e+00   8.48392822e-01   1.16959795e+00   1.09922465e+00\n",
      "    1.15875405e+00   1.13582830e+00   8.28431627e-01   1.13703203e+00\n",
      "    2.21122186e+00   1.70667577e+00   1.88377455e+00   1.71371916e+00\n",
      "    1.97446312e+00   1.99640186e+00   1.53747683e+00   1.78082854e+00\n",
      "    1.72596982e+00   2.14927130e+00   1.69725911e+00   1.69723861e+00\n",
      "    1.82071661e+00   1.75358673e+00   1.99910178e+00   1.92752474e+00\n",
      "    1.67204966e+00   2.06158840e+00   2.17587391e+00   1.43559029e+00\n",
      "    1.84651267e+00   1.67077463e+00   1.81709689e+00   1.49292766e+00\n",
      "    1.74099085e+00   1.58214495e+00   1.47570753e+00   1.48989737e+00\n",
      "    1.78801461e+00   1.44076854e+00   1.67081325e+00   1.66462607e+00\n",
      "    1.84673528e+00   1.34817560e+00   1.42101135e+00   1.86165449e+00\n",
      "    1.92902441e+00   1.56998542e+00   1.48015216e+00   1.67821892e+00\n",
      "    1.91266026e+00   1.73733297e+00   1.63901930e+00   1.89509476e+00\n",
      "    1.97587355e+00   1.78364475e+00   1.58604059e+00   1.62687716e+00\n",
      "    1.84111608e+00   1.54817668e+00]]\n",
      "MSE: 7.86020248866e-06\n",
      "RMSE: 0.0028036052662\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00]\n",
      " [  9.39434908e-04   9.39434908e-04   9.39434908e-04   9.39434908e-04\n",
      "    9.39434908e-04   9.39434908e-04   9.39434908e-04   9.39434908e-04\n",
      "    9.39434908e-04   9.39434908e-04   9.39434908e-04   9.39434908e-04\n",
      "    9.39434908e-04   9.39434908e-04   9.39434908e-04   9.39434908e-04\n",
      "    9.39434908e-04   9.39434908e-04   9.39434908e-04   9.39434908e-04\n",
      "    9.39434908e-04   9.39434908e-04   9.39434908e-04   9.39434908e-04\n",
      "    9.39434908e-04   9.39434908e-04   9.39434908e-04   9.39434908e-04\n",
      "    9.39434908e-04   9.39434908e-04   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.40000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   2.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   2.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   2.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   9.80204725e-01   1.18278595e+00\n",
      "    9.80204725e-01   7.77623495e-01   1.08149534e+00   9.80204725e-01\n",
      "    1.08149534e+00   1.08149534e+00   8.78914110e-01   1.08149534e+00\n",
      "    2.29698272e+00   1.68923903e+00   1.89182026e+00   1.58794841e+00\n",
      "    1.99311087e+00   1.89182026e+00   1.48665780e+00   1.58794841e+00\n",
      "    1.58794841e+00   2.29698272e+00   1.79052964e+00   1.68923903e+00\n",
      "    1.89182026e+00   1.79052964e+00   2.19569210e+00   2.09440149e+00\n",
      "    1.58794841e+00   1.99311087e+00   2.09440149e+00   1.28407657e+00\n",
      "    2.03027412e+00   1.73595823e+00   1.73595823e+00   1.53974764e+00\n",
      "    1.83406353e+00   1.53974764e+00   1.53974764e+00   1.53974764e+00\n",
      "    1.83406353e+00   1.34353704e+00   1.63785293e+00   1.73595823e+00\n",
      "    1.93216882e+00   1.24543175e+00   1.14732645e+00   2.03027412e+00\n",
      "    2.12837942e+00   1.53974764e+00   1.53974764e+00   1.83406353e+00\n",
      "    2.12837942e+00   2.03027412e+00   1.63785293e+00   2.03027412e+00\n",
      "    2.22648471e+00   2.03027412e+00   1.63785293e+00   1.73595823e+00\n",
      "    2.03027412e+00   1.53974764e+00]]\n",
      "MSE: 0.0\n",
      "RMSE: 0.0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jun  8 12:55:08 2017\n",
    "\n",
    "@author: L\n",
    "@attr filter\n",
    "\"\"\"\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model as lm \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "class model_fac:\n",
    "    def __init__(self):\n",
    "        self.name = \"create models for train\"\n",
    "    \n",
    "    def model_set(self):\n",
    "        gbdt = GradientBoostingRegressor()\n",
    "        rf = RandomForestRegressor()\n",
    "        ab = AdaBoostRegressor()\n",
    "        #svr = SVR()\n",
    "        lm1 = lm.BayesianRidge()\n",
    "        lr = LinearRegression(normalize=True)\n",
    "        modelset = {}\n",
    "        modelset['gbdt'] = gbdt\n",
    "        modelset['rf'] = rf\n",
    "        modelset['ab'] = ab\n",
    "        #modelset['svr'] = svr\n",
    "        modelset['lr'] = lr\n",
    "        modelset['lm1'] = lm1\n",
    "        return modelset\n",
    "\n",
    "class attr_filter:\n",
    "    def __init__(self):\n",
    "        self.name = 'fit data and evalute data attr '\n",
    "        self.modelset = {}\n",
    "        self.keys = ''\n",
    "    def models(self):\n",
    "        gbdt = GradientBoostingRegressor()\n",
    "        rf = RandomForestRegressor()\n",
    "        ab = AdaBoostRegressor()\n",
    "#        svr = SVR()\n",
    "        lm1 = lm.BayesianRidge()\n",
    "        lr = LinearRegression(normalize=True)\n",
    "        ridge = Ridge(alpha=7)\n",
    "        lasso = Lasso(alpha=.05)\n",
    "        rlasso = RandomizedLasso(alpha=0.04)\n",
    "        modelset = {}\n",
    "        modelset['gbdt'] = gbdt\n",
    "        modelset['rf'] = rf\n",
    "        modelset['ab'] = ab\n",
    "#        modelset['svr'] = svr\n",
    "        modelset['lm1'] = lm1\n",
    "        modelset['lr'] = lr\n",
    "        modelset['ridge'] = ridge\n",
    "        modelset['lasso'] = lasso\n",
    "        modelset['rlasso'] = rlasso\n",
    "        self.modelset = modelset\n",
    "        \n",
    "    def rank_attr(self,names,X,Y,x_test,y_test):\n",
    "        '''\n",
    "        不同分类器对特征进行排序\n",
    "        '''\n",
    "        self.models()\n",
    "        \n",
    "        keys = self.modelset.keys()\n",
    "        keys = [x for x in keys]\n",
    "        self.keys = keys\n",
    "        if len(names) != len(X[0]):\n",
    "            print('columns error',names)\n",
    "            return 0\n",
    "        \n",
    "        ranks = {}\n",
    "        def rank_to_dict(ranks, names, order=1):\n",
    "            minmax = MinMaxScaler()\n",
    "            ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "            ranks = map(lambda x: round(x, 2), ranks)\n",
    "            return dict(zip(names, ranks ))\n",
    "        for i in range(len(keys)):\n",
    "            model = self.modelset[keys[i]]\n",
    "            model.fit(X, Y)\n",
    "            print('flash model',keys[i])\n",
    "            self.modelset[keys[i]] = model\n",
    "#            try:\n",
    "#                self.test_before_rank(i,X,Y,x_test,y_test)\n",
    "#                self.test_after_rank(i,X,Y,x_test,y_test)\n",
    "#            except:\n",
    "#                continue\n",
    "            try:\n",
    "                ranks[keys[i]] = rank_to_dict(np.abs(model.coef_), names)\n",
    "            except:\n",
    "                try:\n",
    "                    ranks[keys[i]] = rank_to_dict(np.abs(model.scores_), names)\n",
    "                except:\n",
    "                    try:\n",
    "                        ranks[keys[i]] = rank_to_dict(np.abs(model.feature_importances_), names)\n",
    "                    except:\n",
    "                        try:\n",
    "                            ranks[keys[i]] = rank_to_dict(np.abs(model.model.feature_importances_), names)\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "        rankdf = pd.DataFrame.from_dict(ranks)\n",
    "        print(rankdf)\n",
    "        return rankdf\n",
    "    \n",
    "    def test_before_rank(self,i,X,Y,x_test,y_test):\n",
    "        print('test without rank')\n",
    "        pred = self.modelset[self.keys[i]].predict(x_test)\n",
    "        # 用scikit-learn计算MSE\n",
    "        print(\"MSE:\",mean_squared_error(y_test, pred))\n",
    "        # 用scikit-learn计算RMSE\n",
    "        print (\"RMSE:\",np.sqrt(mean_squared_error(y_test, pred)))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def test_after_rank(self,i,X,Y,x_test,y_test):\n",
    "        print('test with rank')\n",
    "        #使用模型训练做为特征选择\n",
    "        from sklearn.feature_selection import SelectFromModel\n",
    "        model = self.modelset[self.keys[i]]\n",
    "        model = SelectFromModel(model, prefit=True)\n",
    "        X = model.transform(X)\n",
    "        #print(X[0,:])\n",
    "        x_test = model.transform(x_test)\n",
    "        new_model = self.modelset[self.keys[i]]\n",
    "        new_model.fit(X,Y)\n",
    "        pred = new_model.predict(x_test)\n",
    "        # 用scikit-learn计算MSE\n",
    "        print(\"MSE:\",mean_squared_error(y_test, pred))\n",
    "        # 用scikit-learn计算RMSE\n",
    "        print (\"RMSE:\",np.sqrt(mean_squared_error(y_test, pred)))\n",
    "            \n",
    "\n",
    "'''\n",
    "数据切分：生成训练集与测试集\n",
    "切分后特征选择：取平均\n",
    "stacking训练：第一层训练\n",
    "'''\n",
    "def data_fold(x_data,y_data,n_splits=3):\n",
    "    '''input : N*D N*1 array\n",
    "       output : TRAIN TEST LIST'''\n",
    "    folded = []\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    for train_index, test_index in kf.split(x_data):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = x_data[train_index], x_data[test_index]\n",
    "        y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "        temp = {}\n",
    "        y_train.shape = (len(y_train),1)\n",
    "        y_test.shape = (len(y_test),1)\n",
    "        temp['train'] = np.hstack((X_train,y_train))\n",
    "        temp['test'] = np.hstack((X_test,y_test))\n",
    "        folded.append(temp)\n",
    "    return folded\n",
    " \n",
    "    \n",
    "def step1(folded,trans=False,**model):\n",
    "    '''\n",
    "    train and test\n",
    "    '''\n",
    "    if len(folded) != len(model):\n",
    "        print('model number is ',len(model),'split dataset for ',len(folded))\n",
    "        return 0\n",
    "    keys = model.keys()\n",
    "    keys = [x for x in keys]\n",
    "    step1_result = [] \n",
    "    for i in range(len(folded)):\n",
    "        train = folded[i]['train']\n",
    "        x_train = train[:,:-1]\n",
    "        y_train = train[:,-1]#lie vec  [:,-1] hang vec\n",
    "        #print('x',x_train)\n",
    "        #print('y',y_train)\n",
    "        #train\n",
    "        model[keys[i]].fit(x_train,y_train)\n",
    "        #test\n",
    "        test = folded[i]['test']\n",
    "        x_test = test[:,:-1]\n",
    "        y_test = test[:,-1]#lie vec  [:,-1] hang vec\n",
    "        if trans:\n",
    "            from sklearn.feature_selection import SelectFromModel\n",
    "            sf = SelectFromModel(model[keys[i]], prefit=True)\n",
    "            x_train = sf.transform(x_train)\n",
    "            x_test = sf.transform(x_test)\n",
    "            model[keys[i]].fit(x_train,y_train)\n",
    "        pred = model[keys[i]].predict(x_test)\n",
    "        #print(y_test)\n",
    "        #print(pred)\n",
    "        step1_result.append(np.stack((y_test,pred),axis=0))\n",
    "        #break\n",
    "    return step1_result\n",
    "\n",
    "def feature_selection():\n",
    "    '''\n",
    "    返回：无\n",
    "    '''\n",
    "    model_num = 5\n",
    "    from sklearn.datasets import load_iris\n",
    "    data = load_iris()\n",
    "    folded = data_fold(data.data,data.target,n_splits=model_num)\n",
    "    names = range(4)\n",
    "    ranklist = []\n",
    "    for i in range(model_num):\n",
    "        train = folded[i]['train']\n",
    "        x_train = train[:,:-1]\n",
    "        y_train = train[:,-1]#lie vec  [:,-1] hang vec\n",
    "        #test\n",
    "        test = folded[i]['test']\n",
    "        x_test = test[:,:-1]\n",
    "        y_test = test[:,-1]\n",
    "        print('before select attr')\n",
    "        model = attr_filter()\n",
    "        rank = model.rank_attr(names,np.array(x_train),np.array(y_train),np.array(x_test),np.array(y_test))\n",
    "        ranklist.append(rank)\n",
    "        \n",
    "    rank = pd.concat(ranklist)\n",
    "    rank.index.name = 'names'\n",
    "    rank.to_csv('attr_rank.csv')\n",
    "    rank = rank.reset_index().groupby('names').apply(np.mean)\n",
    "    rank.loc[:,'mean'] = rank.drop('names',axis=1).mean(axis=1)\n",
    "    print(rank)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    model_num = 5\n",
    "    from sklearn.datasets import load_iris\n",
    "    data = load_iris()\n",
    "    folded = data_fold(data.data,data.target,n_splits=model_num)\n",
    "    #folded\n",
    "    model = model_fac()\n",
    "    modelset = model.model_set()\n",
    "\n",
    "    result = step1(folded,trans=False,**modelset)\n",
    "    #print(result)\n",
    "    result = np.concatenate(result,axis=1)\n",
    "    #result.shape\n",
    "    #print(result)\n",
    "    y = result[:,0]\n",
    "    pred = result[:,1]\n",
    "    # 用scikit-learn计算MSE\n",
    "    print(\"MSE:\",mean_squared_error(y, pred))\n",
    "    # 用scikit-learn计算RMSE\n",
    "    print (\"RMSE:\",np.sqrt(mean_squared_error(y, pred)))\n",
    "\n",
    "    result = step1(folded,trans=True,**modelset)\n",
    "    #print(result)\n",
    "    result = np.concatenate(result,axis=1)\n",
    "    #result.shape\n",
    "    #print(result)\n",
    "    y = result[:,0]\n",
    "    pred = result[:,1]\n",
    "    # 用scikit-learn计算MSE\n",
    "    print(\"MSE:\",mean_squared_error(y, pred))\n",
    "    # 用scikit-learn计算RMSE\n",
    "    print (\"RMSE:\",np.sqrt(mean_squared_error(y, pred)))\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
